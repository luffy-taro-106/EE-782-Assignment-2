{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.10.18)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/luffy_sama/Desktop/Workspace/EE_782/Assignment_2/deepface310/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream, resource_exists\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import pyttsx3\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import threading\n",
        "import mediapipe as mp\n",
        "import time\n",
        "import pygame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------- Configuration --------\n",
        "MODEL_NAME = \"medium.en\"\n",
        "RECORD_SECONDS = 4\n",
        "SAMPLE_RATE = 16000\n",
        "WAKE_PHRASES = [\"guard my room\", \"guard my room please\", \"guard\", \"activate guard\", \"start guard\"]\n",
        "DISARM_PHRASES = [\"stop guard\", \"disarm\", \"deactivate guard\", \"stop guard please\", \"stop\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-13 01:03:50.297 Python[52249:1270605] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ec4aacaa08a473b8df27de1f80ac6ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set default audio input device (change index if needed)\n",
        "try:\n",
        "    sd.default.device = sd.query_devices(kind='input')['name']\n",
        "except Exception as e:\n",
        "    print(\"[audio] Could not set default device:\", e)\n",
        "\n",
        "# persistent webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    print(\"[webcam] ERROR: cannot open webcam\")\n",
        "\n",
        "def record_chunk(seconds=RECORD_SECONDS, samplerate=SAMPLE_RATE):\n",
        "    print(f\"[audio] recording {seconds}s...\")\n",
        "    try:\n",
        "        recording = sd.rec(int(seconds * samplerate), samplerate=samplerate, channels=1)\n",
        "        sd.wait()\n",
        "    except Exception as e:\n",
        "        print(\"[audio] ERROR:\", e)\n",
        "        return None\n",
        "    tmp = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    sf.write(tmp.name, recording, samplerate)\n",
        "    return tmp.name\n",
        "\n",
        "def contains_phrase(text, phrases):\n",
        "    text = text.lower()\n",
        "    return any(p in text for p in phrases)\n",
        "\n",
        "def get_webcam_frame():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        return None\n",
        "    return frame\n",
        "\n",
        "out_widget = widgets.Output()\n",
        "display(out_widget)\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "BASE_DIR = \"enrolled_faces\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def extract_embedding(image_path):\n",
        "    \"\"\"Extract a 3D face landmark embedding from an image.\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"[enroll] Could not read image:\", image_path)\n",
        "        return None\n",
        "\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
        "        results = face_mesh.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        if not results.multi_face_landmarks:\n",
        "            print(\"[enroll] No face found in\", image_path)\n",
        "            return None\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        emb = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()\n",
        "        emb = emb / np.linalg.norm(emb)\n",
        "        return emb\n",
        "\n",
        "def enroll_face(person_name, image_paths):\n",
        "    person_dir = os.path.join(BASE_DIR, person_name)\n",
        "    os.makedirs(person_dir, exist_ok=True)\n",
        "\n",
        "    for i, path in enumerate(image_paths):  # keep enumerate\n",
        "        if not os.path.exists(path):\n",
        "            print(\"[enroll] File not found:\", path)\n",
        "            continue\n",
        "        emb = extract_embedding(path)\n",
        "        if emb is not None:\n",
        "            np.save(os.path.join(person_dir, f\"{i}.npy\"), emb)\n",
        "            print(f\"[enroll] Saved embedding {i+1} for {person_name}\")\n",
        "    print(f\"[enroll] Completed enrollment for {person_name} ✅\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1760297630.616415       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "/Users/luffy_sama/Desktop/Workspace/EE_782/Assignment_2/deepface310/lib/python3.10/site-packages/google/protobuf/symbol_database.py:78: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "I0000 00:00:1760297630.693593       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297630.781897       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[enroll] Saved embedding 1 for Vishal\n",
            "[enroll] Saved embedding 2 for Vishal\n",
            "[enroll] Saved embedding 3 for Vishal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1760297630.855055       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297630.879717       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297630.936049       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297630.980248       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297630.997833       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297631.013210       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297631.029586       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297631.048372       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[enroll] Saved embedding 4 for Vishal\n",
            "[enroll] Saved embedding 5 for Vishal\n",
            "[enroll] Saved embedding 6 for Vishal\n",
            "[enroll] Saved embedding 7 for Vishal\n",
            "[enroll] Completed enrollment for Vishal ✅\n",
            "[enroll] Saved embedding 1 for Darshan\n",
            "[enroll] Saved embedding 2 for Darshan\n",
            "[enroll] Saved embedding 3 for Darshan\n",
            "[enroll] Saved embedding 4 for Darshan\n",
            "[enroll] Saved embedding 5 for Darshan\n",
            "[enroll] Saved embedding 6 for Darshan\n",
            "[enroll] Saved embedding 7 for Darshan\n",
            "[enroll] Completed enrollment for Darshan ✅\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1760297631.064385       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297631.081917       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "I0000 00:00:1760297631.114994       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
          ]
        }
      ],
      "source": [
        "enroll_face(\"Vishal\", [\n",
        "    \"enrolled_faces/vishal1.jpg\",\n",
        "    \"enrolled_faces/vishal2.jpg\",\n",
        "    \"enrolled_faces/vishal3.jpg\",\n",
        "    \"enrolled_faces/vishal4.jpg\",\n",
        "    \"enrolled_faces/vishal7.jpg\",\n",
        "    \"enrolled_faces/vishal8.jpg\",\n",
        "    \"enrolled_faces/vishal9.jpg\"\n",
        "])\n",
        "\n",
        "enroll_face(\"Darshan\", [\n",
        "    \"enrolled_faces/darshan1.jpg\",\n",
        "    \"enrolled_faces/darshan2.jpg\",\n",
        "    \"enrolled_faces/darshan3.jpg\",\n",
        "    \"enrolled_faces/darshan4.jpg\",\n",
        "    \"enrolled_faces/darshan5.jpg\",\n",
        "    \"enrolled_faces/darshan6.jpg\",\n",
        "    \"enrolled_faces/darshan7.jpg\"\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recognize_face_with_score(frame, threshold=0.90 ):\n",
        "    \"\"\"\n",
        "    Recognize face from frame by comparing with all enrolled faces.\n",
        "    Uses majority vote: counts embeddings exceeding threshold per person.\n",
        "    Returns the person with the maximum count.\n",
        "    \"\"\"\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
        "        results = face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if not results.multi_face_landmarks:\n",
        "            return \"Unknown\", frame, 0.0\n",
        "\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        emb = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()\n",
        "        emb = emb / np.linalg.norm(emb)\n",
        "\n",
        "        best_name, best_count, best_sim = \"Unknown\", 0, 0.0\n",
        "\n",
        "        for person_name in os.listdir(BASE_DIR):\n",
        "            person_dir = os.path.join(BASE_DIR, person_name)\n",
        "            if not os.path.isdir(person_dir):\n",
        "                continue\n",
        "\n",
        "\n",
        "            best_sim=0\n",
        "\n",
        "            for f in os.listdir(person_dir):\n",
        "                if not f.endswith(\".npy\"):\n",
        "                    continue\n",
        "                known_emb = np.load(os.path.join(person_dir, f))\n",
        "                sim = cosine_similarity([emb], [known_emb])[0][0]\n",
        "\n",
        "                if sim > best_sim:\n",
        "                    best_sim = sim\n",
        "                    best_name = person_name\n",
        "\n",
        "\n",
        "        if best_sim < threshold:\n",
        "           best_name = \"Unknown\"\n",
        "\n",
        "        return best_name, frame, best_sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "\n",
        "class GuardAgent:\n",
        "    def __init__(self, cap, ws_uri=\"ws://localhost:8765\"):\n",
        "        print(\"[init] loading Whisper model:\", MODEL_NAME)\n",
        "        self.model = whisper.load_model(MODEL_NAME)\n",
        "        self.tts = pyttsx3.init()\n",
        "        self.tts.setProperty(\"rate\", 150)\n",
        "        self.tts.setProperty(\"volume\", 1.0)\n",
        "        self.guard_mode = False\n",
        "        self.cap = cap\n",
        "        self.guard_thread = None\n",
        "        self.guard_stop_event = threading.Event()\n",
        "        self.current_name = \"Unknown\"\n",
        "        self.face_entry_time = {}\n",
        "        self.ws_uri = ws_uri\n",
        "        self.ws_loop = asyncio.new_event_loop()\n",
        "        self.ws_thread = threading.Thread(target=self.ws_loop.run_forever, daemon=True)\n",
        "        self.ws_thread.start()\n",
        "        self.ws_conn = None\n",
        "        asyncio.run_coroutine_threadsafe(self.connect_ws(), self.ws_loop)\n",
        "\n",
        "         # Initialize pygame mixer for alarm\n",
        "        pygame.mixer.init()\n",
        "        self.alarm_playing = False\n",
        "\n",
        "    async def connect_ws(self):\n",
        "        self.ws_conn = await websockets.connect(self.ws_uri)\n",
        "        print(\"[ws] Connected to WebSocket server\")\n",
        "\n",
        "\n",
        "    def listen_and_toggle(self):\n",
        "        # Record audio chunk\n",
        "        wav_path = record_chunk()  # assume this function exists and returns a path to the wav file\n",
        "        if not wav_path:\n",
        "            return\n",
        "\n",
        "        # Transcribe the audio\n",
        "        try:\n",
        "            result = self.model.transcribe(wav_path, language=\"en\")\n",
        "            transcript = result.get(\"text\", \"\").strip().lower()\n",
        "        except Exception as e:\n",
        "            print(\"[transcribe] error:\", e)\n",
        "            transcript = \"\"\n",
        "        finally:\n",
        "            os.remove(wav_path)\n",
        "\n",
        "        print(\"[heard]\", transcript)\n",
        "\n",
        "        # Activate guard mode if wake phrase is detected\n",
        "        if not self.guard_mode and contains_phrase(transcript, WAKE_PHRASES):\n",
        "            self.guard_mode = True\n",
        "            self.tts.say(\"Guard mode activated.\")\n",
        "            self.tts.runAndWait()\n",
        "            self.guard_stop_event.clear()\n",
        "            self.guard_thread = threading.Thread(target=self.run_guard_mode, daemon=True)\n",
        "            self.guard_thread.start()\n",
        "\n",
        "        # Deactivate guard mode if disarm phrase is detected\n",
        "        elif self.guard_mode and contains_phrase(transcript, DISARM_PHRASES):\n",
        "            self.guard_mode = False\n",
        "            self.tts.say(\"Guard mode deactivated.\")\n",
        "            self.tts.runAndWait()\n",
        "            self.guard_stop_event.set()\n",
        "            if self.guard_thread:\n",
        "                self.guard_thread.join()\n",
        "\n",
        "    async def send_ws(self, data):\n",
        "        if self.ws_conn:\n",
        "            await self.ws_conn.send(json.dumps(data))\n",
        "\n",
        "    def run_guard_mode(self):\n",
        "        import mediapipe as mp\n",
        "        import cv2\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, clear_output\n",
        "\n",
        "        print(\"[guard] Running face detection...\")\n",
        "        mp_face = mp.solutions.face_detection\n",
        "        plt.ion()\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        with mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5) as detector:\n",
        "            while self.guard_mode and not self.guard_stop_event.is_set():\n",
        "                ret, frame = self.cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                results = detector.process(rgb)\n",
        "\n",
        "                if results.detections:\n",
        "                    for det in results.detections:\n",
        "                        bboxC = det.location_data.relative_bounding_box\n",
        "                        ih, iw, _ = frame.shape\n",
        "                        x1 = int(bboxC.xmin * iw)\n",
        "                        y1 = int(bboxC.ymin * ih)\n",
        "                        w = int(bboxC.width * iw)\n",
        "                        h = int(bboxC.height * ih)\n",
        "                        x2, y2 = x1 + w, y1 + h\n",
        "\n",
        "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                        face_crop = frame[max(0, y1):min(ih, y2), max(0, x1):min(iw, x2)]\n",
        "\n",
        "                        if face_crop.size > 0:\n",
        "                            name, _, _ = recognize_face_with_score(face_crop)\n",
        "\n",
        "                        self.current_name = name\n",
        "\n",
        "                        # Trigger TTS only once for unknown faces\n",
        "                        if name == \"Unknown\" and name not in self.face_entry_time:\n",
        "                            self.tts.say(\"You have entered an unauthorized area. Please state your purpose.\")\n",
        "                            self.tts.runAndWait()\n",
        "\n",
        "\n",
        "                        if name not in self.face_entry_time:\n",
        "                            self.face_entry_time[name] = time.time()\n",
        "\n",
        "                        duration = time.time() - self.face_entry_time[name]\n",
        "\n",
        "                        if name == \"Unknown\" and duration > 120:\n",
        "                           self.tts.say(\"Unauthorized person found for more than 20 seconds. Starting alarm.\")\n",
        "\n",
        "                        # Alarm trigger for unknown > 25 seconds\n",
        "                        if name == \"Unknown\" and duration > 125:\n",
        "                            if not self.alarm_playing:\n",
        "                                pygame.mixer.music.load(\"alarm.mp3\")\n",
        "                                pygame.mixer.music.play(-1)  # loop indefinitely\n",
        "                                self.alarm_playing = True\n",
        "                        else:\n",
        "                            # Stop alarm if authorized or no face\n",
        "                            if self.alarm_playing:\n",
        "                                pygame.mixer.music.stop()\n",
        "                                self.alarm_playing = False\n",
        "\n",
        "\n",
        "                        if name not in self.face_entry_time:\n",
        "                            self.face_entry_time[name] = time.time()\n",
        "\n",
        "                        duration = time.time() - self.face_entry_time[name]\n",
        "\n",
        "                        cv2.putText(frame, f\"{name}\", (x1, y2 + 30),\n",
        "                                    cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
        "                                    (0, 255, 0) if name != \"Unknown\" else (0, 0, 255), 2)\n",
        "\n",
        "                        # Send data to WebSocket\n",
        "                        data = {\n",
        "                            \"current_name\": name,\n",
        "                            \"entry_time\": self.face_entry_time[name],\n",
        "                            \"current_time\": time.time(),\n",
        "                            \"duration\": duration\n",
        "                        }\n",
        "                        asyncio.run_coroutine_threadsafe(self.send_ws(data), self.ws_loop)\n",
        "\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                ax.clear()\n",
        "                ax.imshow(frame_rgb)\n",
        "                ax.axis('off')\n",
        "                clear_output(wait=True)\n",
        "                display(fig)\n",
        "                plt.pause(0.01)\n",
        "\n",
        "        plt.close(fig)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] loading Whisper model: medium.en\n",
            "Say 'guard my room' to activate. Say 'stop guard' to stop.\n",
            "[audio] recording 4s...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/luffy_sama/Desktop/Workspace/EE_782/Assignment_2/deepface310/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[exit] KeyboardInterrupt received — cleaning up.\n",
            "[exit] done.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"[webcam] cannot open webcam\")\n",
        "        return\n",
        "    agent = GuardAgent(cap)\n",
        "    print(\"Say 'guard my room' to activate. Say 'stop guard' to stop.\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            agent.listen_and_toggle()\n",
        "            # Optional: show webcam preview when not in guard mode\n",
        "            if not agent.guard_mode:\n",
        "                frame = get_webcam_frame()\n",
        "                if frame is not None:\n",
        "                    _, encoded_img = cv2.imencode('.png', frame)\n",
        "                    img_bytes = encoded_img.tobytes()\n",
        "                    with out_widget:\n",
        "                        clear_output(wait=True)\n",
        "                        display(widgets.Image(value=img_bytes, format='png', width=640, height=480))\n",
        "            time.sleep(0.1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n[exit] KeyboardInterrupt received — cleaning up.\")\n",
        "    finally:\n",
        "        if hasattr(agent, 'alarm_playing') and agent.alarm_playing:\n",
        "            pygame.mixer.music.stop()\n",
        "            agent.alarm_playing = False\n",
        "        cap.release()\n",
        "        print(\"[exit] done.\")\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cv2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m         cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[exit] done.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[webcam] cannot open webcam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"[webcam] cannot open webcam\")\n",
        "        return\n",
        "\n",
        "    agent = GuardAgent(cap)\n",
        "\n",
        "    # Force guard mode ON immediately\n",
        "    agent.guard_mode = True\n",
        "    agent.guard_stop_event.clear()\n",
        "    agent.guard_thread = threading.Thread(target=agent.run_guard_mode, daemon=True)\n",
        "    agent.guard_thread.start()\n",
        "\n",
        "    print(\"[init] Guard mode activated automatically.\")\n",
        "    print(\"Say 'stop guard' to stop.\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if not agent.guard_mode:\n",
        "                frame = get_webcam_frame()\n",
        "                if frame is not None:\n",
        "                    _, encoded_img = cv2.imencode('.png', frame)\n",
        "                    img_bytes = encoded_img.tobytes()\n",
        "                    with out_widget:\n",
        "                        clear_output(wait=True)\n",
        "                        display(widgets.Image(value=img_bytes, format='png', width=640, height=480))\n",
        "\n",
        "            time.sleep(0.1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n[exit] KeyboardInterrupt received — cleaning up.\")\n",
        "    finally:\n",
        "        if hasattr(agent, 'alarm_playing') and agent.alarm_playing:\n",
        "            pygame.mixer.music.stop()\n",
        "            agent.alarm_playing = False\n",
        "        cap.release()\n",
        "        print(\"[exit] done.\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepface310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
