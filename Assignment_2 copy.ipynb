{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import whisper\n",
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import pyttsx3\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------- Configuration --------\n",
        "MODEL_NAME = \"medium.en\"\n",
        "RECORD_SECONDS = 4\n",
        "SAMPLE_RATE = 16000\n",
        "WAKE_PHRASES = [\"guard my room\", \"guard my room please\", \"guard\", \"activate guard\", \"start guard\"]\n",
        "DISARM_PHRASES = [\"stop guard\", \"disarm\", \"deactivate guard\", \"stop guard please\", \"stop\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4f925d11f1141529cd98930f3cfe515",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set default audio input device (change index if needed)\n",
        "try:\n",
        "    sd.default.device = sd.query_devices(kind='input')['name']\n",
        "except Exception as e:\n",
        "    print(\"[audio] Could not set default device:\", e)\n",
        "\n",
        "# persistent webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    print(\"[webcam] ERROR: cannot open webcam\")\n",
        "\n",
        "def record_chunk(seconds=RECORD_SECONDS, samplerate=SAMPLE_RATE):\n",
        "    print(f\"[audio] recording {seconds}s...\")\n",
        "    try:\n",
        "        recording = sd.rec(int(seconds * samplerate), samplerate=samplerate, channels=1)\n",
        "        sd.wait()\n",
        "    except Exception as e:\n",
        "        print(\"[audio] ERROR:\", e)\n",
        "        return None\n",
        "    tmp = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    sf.write(tmp.name, recording, samplerate)\n",
        "    return tmp.name\n",
        "\n",
        "def contains_phrase(text, phrases):\n",
        "    text = text.lower()\n",
        "    return any(p in text for p in phrases)\n",
        "\n",
        "def get_webcam_frame():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        return None\n",
        "    return frame\n",
        "\n",
        "out_widget = widgets.Output()\n",
        "display(out_widget)\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     global cap\n",
        "#     cap = cv2.VideoCapture(0)\n",
        "#     if not cap.isOpened():\n",
        "#         print(\"[webcam] ERROR: cannot open webcam\")\n",
        "#         return\n",
        "\n",
        "#     agent = GuardAgent()\n",
        "#     print(\"Say a wake phrase (e.g., 'Guard my room') to activate. Say a disarm phrase to stop.\")\n",
        "\n",
        "#     try:\n",
        "#         while True:\n",
        "#             agent.listen_and_toggle()\n",
        "\n",
        "#             if not agent.guard_mode:\n",
        "#                 frame = get_webcam_frame()\n",
        "#                 if frame is not None:\n",
        "#                     _, encoded_img = cv2.imencode('.png', frame)\n",
        "#                     img_bytes = encoded_img.tobytes()\n",
        "#                     with out_widget:\n",
        "#                         clear_output(wait=True)\n",
        "#                         display(widgets.Image(value=img_bytes, format='png', width=640, height=480))\n",
        "#             time.sleep(0.1)\n",
        "\n",
        "#     except KeyboardInterrupt:\n",
        "#         print(\"\\n[exit] KeyboardInterrupt received — cleaning up.\")\n",
        "#     finally:\n",
        "#         cap.release()\n",
        "#         print(\"[exit] done.\")\n",
        "\n",
        "# main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "BASE_DIR = \"enrolled_faces\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def extract_embedding(image_path):\n",
        "    \"\"\"Extract a 3D face landmark embedding from an image.\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(\"[enroll] Could not read image:\", image_path)\n",
        "        return None\n",
        "\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
        "        results = face_mesh.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        if not results.multi_face_landmarks:\n",
        "            print(\"[enroll] No face found in\", image_path)\n",
        "            return None\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        emb = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()\n",
        "        emb = emb / np.linalg.norm(emb)\n",
        "        return emb\n",
        "\n",
        "def enroll_face(person_name, image_paths):\n",
        "    \"\"\"\n",
        "    Enroll one person using multiple images.\n",
        "    Creates a folder for that person and saves one .npy file per image.\n",
        "    \"\"\"\n",
        "    person_dir = os.path.join(BASE_DIR, person_name)\n",
        "    os.makedirs(person_dir, exist_ok=True)\n",
        "\n",
        "    for i, path in enumerate(image_paths):\n",
        "        emb = extract_embedding(path)\n",
        "        if emb is not None:\n",
        "            np.save(os.path.join(person_dir, f\"{i}.npy\"), emb)\n",
        "            print(f\"[enroll] Saved embedding {i+1} for {person_name}\")\n",
        "    print(f\"[enroll] Completed enrollment for {person_name} ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[enroll] Could not read image: darshan1.jpg\n",
            "[enroll] Could not read image: darshan2.jpg\n",
            "[enroll] Could not read image: darshan3.jpg\n",
            "[enroll] Completed enrollment for darshan ✅\n",
            "[enroll] Could not read image: roommate1.jpg\n",
            "[enroll] Could not read image: roommate2.jpg\n",
            "[enroll] Completed enrollment for roommate ✅\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@1923.188] global loadsave.cpp:268 findDecoder imread_('darshan1.jpg'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@1923.189] global loadsave.cpp:268 findDecoder imread_('darshan2.jpg'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@1923.189] global loadsave.cpp:268 findDecoder imread_('darshan3.jpg'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@1923.189] global loadsave.cpp:268 findDecoder imread_('roommate1.jpg'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@1923.189] global loadsave.cpp:268 findDecoder imread_('roommate2.jpg'): can't open/read file: check file path/integrity\n"
          ]
        }
      ],
      "source": [
        "enroll_face(\"darshan\", [\"darshan1.jpg\", \"darshan2.jpg\", \"darshan3.jpg\"])\n",
        "enroll_face(\"roommate\", [\"roommate1.jpg\", \"roommate2.jpg\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recognize_face_with_score(frame):\n",
        "    \"\"\"\n",
        "    Recognize face from frame by comparing with all enrolled faces.\n",
        "    Averages similarity scores per person.\n",
        "    \"\"\"\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
        "        results = face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if not results.multi_face_landmarks:\n",
        "            return \"Unknown\", frame, 0\n",
        "\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        emb = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()\n",
        "        emb = emb / np.linalg.norm(emb)\n",
        "\n",
        "        best_name, best_sim = \"Unknown\", 0.0\n",
        "\n",
        "        for person_name in os.listdir(BASE_DIR):\n",
        "            person_dir = os.path.join(BASE_DIR, person_name)\n",
        "            if not os.path.isdir(person_dir):\n",
        "                continue\n",
        "\n",
        "            sims = []\n",
        "            for f in os.listdir(person_dir):\n",
        "                if not f.endswith(\".npy\"):\n",
        "                    continue\n",
        "                known_emb = np.load(os.path.join(person_dir, f))\n",
        "                sim = cosine_similarity([emb], [known_emb])[0][0]\n",
        "                sims.append(sim)\n",
        "\n",
        "            if sims:\n",
        "                avg_sim = np.mean(sims)\n",
        "                if avg_sim > best_sim:\n",
        "                    best_sim = avg_sim\n",
        "                    best_name = person_name\n",
        "\n",
        "        # Apply threshold\n",
        "        if best_sim < 0.90:\n",
        "            best_name = \"Unknown\"\n",
        "\n",
        "        return best_name, frame, best_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import mediapipe as mp\n",
        "import time\n",
        "\n",
        "class GuardAgent:\n",
        "    def __init__(self, cap):\n",
        "        print(\"[init] loading Whisper model:\", MODEL_NAME)\n",
        "        self.model = whisper.load_model(MODEL_NAME)\n",
        "        self.tts = pyttsx3.init()\n",
        "        self.tts.setProperty(\"rate\", 150)\n",
        "        self.guard_mode = False\n",
        "        self.cap = cap\n",
        "        self.last_name = None\n",
        "        self.last_detect_time = 0\n",
        "        self.guard_thread = None\n",
        "        self.guard_stop_event = threading.Event()\n",
        "\n",
        "    def listen_and_toggle(self):\n",
        "        wav_path = record_chunk()\n",
        "        if not wav_path: return\n",
        "        try:\n",
        "            result = self.model.transcribe(wav_path, language=\"en\")\n",
        "            transcript = result.get(\"text\", \"\").strip().lower()\n",
        "        except Exception as e:\n",
        "            print(\"[transcribe] error:\", e)\n",
        "            transcript = \"\"\n",
        "        finally:\n",
        "            os.remove(wav_path)\n",
        "\n",
        "        print(\"[heard]\", transcript)\n",
        "\n",
        "        if not self.guard_mode and contains_phrase(transcript, WAKE_PHRASES):\n",
        "            self.guard_mode = True\n",
        "            self.tts.say(\"Guard mode activated.\")\n",
        "            self.tts.runAndWait()\n",
        "            self.guard_stop_event.clear()\n",
        "            self.guard_thread = threading.Thread(target=self.run_guard_mode, daemon=True)\n",
        "            self.guard_thread.start()\n",
        "\n",
        "        elif self.guard_mode and contains_phrase(transcript, DISARM_PHRASES):\n",
        "            self.guard_mode = False\n",
        "            self.tts.say(\"Guard mode deactivated.\")\n",
        "            self.tts.runAndWait()\n",
        "            self.guard_stop_event.set()\n",
        "            if self.guard_thread:\n",
        "                self.guard_thread.join()\n",
        "\n",
        "    def run_guard_mode(self):\n",
        "        print(\"[guard] Running face detection...\")\n",
        "        mp_face = mp.solutions.face_detection\n",
        "        plt.ion()\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        with mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.5) as detector:\n",
        "            while self.guard_mode and not self.guard_stop_event.is_set():\n",
        "                ret, frame = self.cap.read()\n",
        "                if not ret: break\n",
        "\n",
        "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                results = detector.process(rgb)\n",
        "\n",
        "                if results.detections:\n",
        "                    for det in results.detections:\n",
        "                        bboxC = det.location_data.relative_bounding_box\n",
        "                        ih, iw, _ = frame.shape\n",
        "                        x1 = int(bboxC.xmin * iw)\n",
        "                        y1 = int(bboxC.ymin * ih)\n",
        "                        w = int(bboxC.width * iw)\n",
        "                        h = int(bboxC.height * ih)\n",
        "                        x2 = x1 + w\n",
        "                        y2 = y1 + h\n",
        "\n",
        "                        # Draw rectangle\n",
        "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "\n",
        "                        # Crop face for recognition\n",
        "                        face_crop = frame[max(0,y1):min(ih,y2), max(0,x1):min(iw,x2)]\n",
        "                        name = \"Chamar\"\n",
        "                        if face_crop.size > 0:\n",
        "                            rec_name, _, sim = recognize_face_with_score(face_crop)\n",
        "                            name = rec_name\n",
        "\n",
        "                            # Welcome back logic\n",
        "                            if name != \"Chamar\" and name != self.last_name:\n",
        "                                if time.time() - self.last_detect_time > 5:  # avoid repeated announcements\n",
        "                                    self.tts.say(f\"Welcome back, {name}.\")\n",
        "                                    self.tts.runAndWait()\n",
        "                                    self.last_name = name\n",
        "                                    self.last_detect_time = time.time()\n",
        "\n",
        "                        # Draw name below rectangle\n",
        "                        cv2.putText(frame, f\"{name}\", (x1, y2 + 30),\n",
        "                                    cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
        "                                    (0,255,0) if name != \"Chamar\" else (0,0,255), 2)\n",
        "\n",
        "                # Display frame in Jupyter\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                ax.clear()\n",
        "                ax.imshow(frame_rgb)\n",
        "                ax.axis('off')\n",
        "                clear_output(wait=True)\n",
        "                display(fig)\n",
        "                plt.pause(0.01)\n",
        "\n",
        "        plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"[webcam] cannot open webcam\")\n",
        "        return\n",
        "    agent = GuardAgent(cap)\n",
        "    print(\"Say 'guard my room' to activate. Say 'stop guard' to stop.\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            agent.listen_and_toggle()\n",
        "            # Optional: show webcam preview when not in guard mode\n",
        "            if not agent.guard_mode:\n",
        "                frame = get_webcam_frame()\n",
        "                if frame is not None:\n",
        "                    _, encoded_img = cv2.imencode('.png', frame)\n",
        "                    img_bytes = encoded_img.tobytes()\n",
        "                    with out_widget:\n",
        "                        clear_output(wait=True)\n",
        "                        display(widgets.Image(value=img_bytes, format='png', width=640, height=480))\n",
        "            time.sleep(0.1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n[exit] KeyboardInterrupt received — cleaning up.\")\n",
        "    finally:\n",
        "        cap.release()\n",
        "        print(\"[exit] done.\")\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepface310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
